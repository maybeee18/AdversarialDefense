{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CleverhansTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMO0BHrL98Vw4mB4h6PBJ71",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttchengab/AdversarialDefense/blob/master/CleverhansTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT4dPiETNE7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7dd7146b-74ad-4062-9f4f-87c80df3e228"
      },
      "source": [
        "!pip uninstall -y tensorflow-addons\n",
        "!pip uninstall -y tensorflow-datasets\n",
        "!pip uninstall -y tensorflow-estimator\n",
        "!pip uninstall -y tensorflow-gcs-config\n",
        "!pip uninstall -y tensorflow-hub\n",
        "!pip uninstall -y tensorflow-privacy\n",
        "!pip uninstall -y tensorflow-metadata\n",
        "!pip uninstall -y tensorflow-probability\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.14\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-addons-0.8.3:\n",
            "  Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Uninstalling tensorflow-datasets-2.1.0:\n",
            "  Successfully uninstalled tensorflow-datasets-2.1.0\n",
            "Uninstalling tensorflow-estimator-2.2.0:\n",
            "  Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Uninstalling tensorflow-gcs-config-2.2.0:\n",
            "  Successfully uninstalled tensorflow-gcs-config-2.2.0\n",
            "Uninstalling tensorflow-hub-0.8.0:\n",
            "  Successfully uninstalled tensorflow-hub-0.8.0\n",
            "Uninstalling tensorflow-privacy-0.2.2:\n",
            "  Successfully uninstalled tensorflow-privacy-0.2.2\n",
            "Uninstalling tensorflow-metadata-0.22.2:\n",
            "  Successfully uninstalled tensorflow-metadata-0.22.2\n",
            "Uninstalling tensorflow-probability-0.10.0:\n",
            "  Successfully uninstalled tensorflow-probability-0.10.0\n",
            "Uninstalling tensorflow-2.2.0:\n",
            "  Successfully uninstalled tensorflow-2.2.0\n",
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.9.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.12.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.30.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (49.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting cleverhans\n",
            "  Cloning https://github.com/tensorflow/cleverhans.git to /tmp/pip-install-eb4wp_kq/cleverhans\n",
            "  Running command git clone -q https://github.com/tensorflow/cleverhans.git /tmp/pip-install-eb4wp_kq/cleverhans\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 2.8MB/s \n",
            "\u001b[?25hCollecting pycodestyle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.2.2)\n",
            "Collecting mnist~=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.18.5)\n",
            "Collecting tensorflow-probability\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/61/800c19c1d586b1e06dc9d645f87c158aadf74233d9d03005d2fbef8a2c04/tensorflow_probability-0.10.1-py2.py3-none-any.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.15.0)\n",
            "Building wheels for collected packages: cleverhans\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cleverhans: filename=cleverhans-3.0.1-cp36-none-any.whl size=262573 sha256=f74be4b558d313b425b2bb17e8549c276eb4dc62b08c6b1fe48980ba1dca38f2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cdc93oaq/wheels/6e/59/ec/723a6f654aaf62c8c40f0f0850fdf71a4948598697f56c3bfa\n",
            "Successfully built cleverhans\n",
            "Installing collected packages: nose, pycodestyle, mnist, tensorflow-probability, cleverhans\n",
            "Successfully installed cleverhans-3.0.1 mnist-0.2.2 nose-1.3.7 pycodestyle-2.6.0 tensorflow-probability-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbZPnX53NMSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.compat import flags\n",
        "from cleverhans.model import CallableModelWrapper\n",
        "from cleverhans.utils import AccuracyReport\n",
        "from cleverhans.utils_pytorch import convert_pytorch_model_to_tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RoumovAampC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TFG7ecwZvUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_size:int, hidden_size:int, out_size:int):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_size, hidden_size, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(hidden_size, out_size, 3, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(out_size)\n",
        "    \n",
        "    def convblock(self, x):\n",
        "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
        "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x): return x + self.convblock(x) # skip connection\n",
        "\n",
        "class ResNet1(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 16)\n",
        "        self.res2 = ResBlock(16, 32, 16)\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #16x28x28\n",
        "        x = self.res2(x) \n",
        "        #16x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #16x14x14\n",
        "        x = x.view(-1, 16*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class ResNet2(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 12)\n",
        "        self.res2 = ResBlock(12, 16, 12)\n",
        "        self.fc1 = nn.Linear(12 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #32x28x28\n",
        "        x = self.res2(x) \n",
        "        #16x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #16x14x14\n",
        "        x = x.view(-1, 12*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class ResNet3(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 32)\n",
        "        self.fc1 = nn.Linear(32 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #32x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #32x14x14\n",
        "        x = x.view(-1, 32*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class ResNet4(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 16)\n",
        "        self.res2 = ResBlock(16, 32, 16)\n",
        "        self.res3 = ResBlock(16, 8, 16)\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #16x28x28\n",
        "        x = self.res2(x) \n",
        "        #32x28x28\n",
        "        x = self.res3(x)\n",
        "        #16x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #16x14x14\n",
        "        x = x.view(-1, 16*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "class LeNet5(torch.nn.Module):          \n",
        "     \n",
        "    def __init__(self):     \n",
        "        super(LeNet5, self).__init__()\n",
        "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5, padding=2)\n",
        "        # Max-pooling\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "        # Convolution\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)   \n",
        "        self.fc2 = nn.Linear(120, 84)       \n",
        "        self.fc3 = nn.Linear(84, 10)    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = F.relu(self.conv1(x))  \n",
        "        # max-pooling with 2x2 grid \n",
        "        x = F.max_pool2d(x, 2) \n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # max-pooling with 2x2 grid\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
        "        # read through https://stackoverflow.com/a/42482819/7551231\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        # FC-1, then perform ReLU non-linearity\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # FC-2, then perform ReLU non-linearity\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # FC-3\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class PytorchMnistModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PytorchMnistModel, self).__init__()\n",
        "    # input is 28x28\n",
        "    # padding=2 for same padding\n",
        "    self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
        "    # feature map size is 14*14 by pooling\n",
        "    # padding=2 for same padding\n",
        "    self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "    # feature map size is 7*7 by pooling\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = x.view(-1, 64 * 7 * 7)  # reshape Variable\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "   def __init__(self):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.conv11 = nn.Conv2d(1, 64, 3)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
        "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
        "        self.fc1 = nn.Linear(128 * 2 * 2, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "   def forward(self, x):\n",
        "       #1, 28, 28\n",
        "       x = F.relu(self.conv11(x))\n",
        "       #64, 26, 26\n",
        "       x = F.relu(self.conv12(x))\n",
        "       #64, 24, 24\n",
        "       x = F.max_pool2d(x, (2,2))\n",
        "       #64, 12, 12\n",
        "       x = F.relu(self.conv21(x))\n",
        "       #128, 10, 10\n",
        "       x = F.relu(self.conv22(x))\n",
        "       #128, 8, 8\n",
        "       x = F.max_pool2d(x, (2,2))\n",
        "       #128, 4, 4\n",
        "       x = F.max_pool2d(x, (2,2))\n",
        "       #128, 2, 2\n",
        "       x = x.view(-1, 128 * 2 * 2)\n",
        "       x = F.relu(self.fc1(x))\n",
        "       x = self.fc2(x)\n",
        "       return F.log_softmax(x, dim=-1)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLpswmDdb--O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "57d9bed7-6bcb-4965-db0f-b6bae89d5e18"
      },
      "source": [
        "# Summary to check torch model structure\n",
        "from torchsummary import summary\n",
        "resnet = ResNet4()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "resnet = resnet.to(device)\n",
        "summary(resnet, input_size=(1, 28, 28))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "            Conv2d-3           [-1, 16, 28, 28]           1,168\n",
            "       BatchNorm2d-4           [-1, 16, 28, 28]              32\n",
            "          ResBlock-5           [-1, 16, 28, 28]               0\n",
            "            Conv2d-6           [-1, 32, 28, 28]           4,640\n",
            "       BatchNorm2d-7           [-1, 32, 28, 28]              64\n",
            "            Conv2d-8           [-1, 16, 28, 28]           4,624\n",
            "       BatchNorm2d-9           [-1, 16, 28, 28]              32\n",
            "         ResBlock-10           [-1, 16, 28, 28]               0\n",
            "           Conv2d-11            [-1, 8, 28, 28]           1,160\n",
            "      BatchNorm2d-12            [-1, 8, 28, 28]              16\n",
            "           Conv2d-13           [-1, 16, 28, 28]           1,168\n",
            "      BatchNorm2d-14           [-1, 16, 28, 28]              32\n",
            "         ResBlock-15           [-1, 16, 28, 28]               0\n",
            "           Linear-16                 [-1, 1024]       3,212,288\n",
            "           Linear-17                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 3,235,570\n",
            "Trainable params: 3,235,570\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.44\n",
            "Params size (MB): 12.34\n",
            "Estimated Total Size (MB): 13.79\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSdErV8LNT8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = flags.FLAGS\n",
        "NB_EPOCHS = 2\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = .001\n",
        "\n",
        "def train(torch_model, train_loader, test_loader,\n",
        "        nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE, train_end=-1, test_end=-1, learning_rate=LEARNING_RATE):\n",
        "\n",
        "\n",
        "    # Truncate the datasets so that our test run more quickly\n",
        "  #   train_loader.dataset.train_data = train_loader.dataset.train_data[:train_end]\n",
        "  #   test_loader.dataset.test_data = test_loader.dataset.test_data[:test_end]\n",
        "\n",
        "    # Train our model\n",
        "    optimizer = optim.Adam(torch_model.parameters(), lr=learning_rate)\n",
        "    train_loss = []\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    step = 0\n",
        "    # breakstep = 0\n",
        "    for _epoch in range(nb_epochs):\n",
        "      # if breakstep == 2:\n",
        "      #     # print(\"break all!\")\n",
        "      #     break\n",
        "      for xs, ys in train_loader:\n",
        "        xs, ys = Variable(xs), Variable(ys)\n",
        "        if torch.cuda.is_available():\n",
        "          xs, ys = xs.cuda(), ys.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        preds = torch_model(xs)\n",
        "        loss = F.nll_loss(preds, ys)\n",
        "        loss.backward()  # calc gradients\n",
        "        train_loss.append(loss.data.item())\n",
        "        optimizer.step()  # update gradients\n",
        "\n",
        "        preds_np = preds.cpu().detach().numpy()\n",
        "        correct += (np.argmax(preds_np, axis=1) == ys.cpu().detach().numpy()).sum()\n",
        "        total += train_loader.batch_size\n",
        "        step += 1\n",
        "        if total % 1000 == 0:\n",
        "          acc = float(correct) / total\n",
        "          print('[%s] Training accuracy: %.2f%%' % (step, acc * 100))\n",
        "          total = 0\n",
        "          correct = 0\n",
        "          # breakstep += 1\n",
        "          # if breakstep == 2:\n",
        "          #     # print(\"break!\")\n",
        "          #     break\n",
        "    \n",
        "def Ensembler(preds):\n",
        "    finalPred = np.zeros(len(preds[0]))\n",
        "    for i in range(len(preds[0])):\n",
        "      scoreList = np.zeros(10)\n",
        "      for pred in preds:\n",
        "        scoreList[pred[i]] += 1\n",
        "      finalPred[i] = np.argmax(scoreList)\n",
        "    return finalPred\n",
        "\n",
        "def eval(model1, model2, model3, model4, test_loader, report, singleModel):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for xs, ys in test_loader:\n",
        "      xs, ys = Variable(xs), Variable(ys)\n",
        "      if torch.cuda.is_available():\n",
        "        xs, ys = xs.cuda(), ys.cuda()\n",
        "\n",
        "      preds1 = model1(xs)\n",
        "      preds_np1 = preds1.cpu().detach().numpy()\n",
        "      preds2 = model2(xs)\n",
        "      preds_np2 = preds2.cpu().detach().numpy()\n",
        "      preds3 = model3(xs)\n",
        "      preds_np3 = preds3.cpu().detach().numpy()\n",
        "      preds4 = model4(xs)\n",
        "      preds_np4 = preds4.cpu().detach().numpy()\n",
        "\n",
        "      #preds for 3 and 4\n",
        "      # preds = [np.argmax(preds_np1, axis=1), np.argmax(preds_np2, axis=1), np.argmax(preds_np3, axis=1)]\n",
        "      preds = [np.argmax(preds_np1, axis=1), np.argmax(preds_np2, axis=1), np.argmax(preds_np3, axis=1), np.argmax(preds_np4, axis=1)]\n",
        "      if not singleModel:\n",
        "        finalPred = Ensembler(preds)\n",
        "      else:\n",
        "        finalPred = np.argmax(preds_np1, axis=1)\n",
        "      correct += (finalPred == ys.cpu().detach().numpy()).sum()\n",
        "      total += len(xs)\n",
        "\n",
        "    acc = float(correct) / total\n",
        "    report.clean_train_clean_eval = acc\n",
        "    print('Clean accuracy: %.2f%%' % (acc * 100))\n",
        "    return report\n",
        "\n",
        "\n",
        "def AttackOnModel(fgsm_model, test_model1, test_model2, test_model3, test_loader, report, singleModel):\n",
        "    # We use tf for evaluation on adversarial data\n",
        "    sess = tf.Session()\n",
        "    x_op = tf.placeholder(tf.float32, shape=(None, 1, 28, 28,))\n",
        "\n",
        "    # Convert pytorch model to a tf_model and wrap it in cleverhans\n",
        "    tf_model_fn = convert_pytorch_model_to_tf(fgsm_model)\n",
        "    cleverhans_model = CallableModelWrapper(tf_model_fn, output_layer='logits')\n",
        "\n",
        "    # Convert the testing models\n",
        "    test_model_fn1 = convert_pytorch_model_to_tf(test_model1)\n",
        "    test_model_fn2 = convert_pytorch_model_to_tf(test_model2)\n",
        "    test_model_fn3 = convert_pytorch_model_to_tf(test_model3)\n",
        "\n",
        "    # Create an FGSM attack\n",
        "    fgsm_op = FastGradientMethod(cleverhans_model, sess=sess)\n",
        "    fgsm_params = {'eps': 0.3,\n",
        "                   'clip_min': 0.,\n",
        "                   'clip_max': 1.}\n",
        "    adv_x_op = fgsm_op.generate(x_op, **fgsm_params)\n",
        "    adv_preds_op0 = tf_model_fn(adv_x_op)\n",
        "    adv_preds_op1 = test_model_fn1(adv_x_op)\n",
        "    adv_preds_op2 = test_model_fn2(adv_x_op)\n",
        "    adv_preds_op3 = test_model_fn3(adv_x_op)\n",
        "\n",
        "    # Run an evaluation of our model against fgsm\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for xs, ys in test_loader:\n",
        "      adv_preds0 = sess.run(adv_preds_op0, feed_dict={x_op: xs})\n",
        "      adv_preds1 = sess.run(adv_preds_op1, feed_dict={x_op: xs})\n",
        "      adv_preds2 = sess.run(adv_preds_op2, feed_dict={x_op: xs})\n",
        "      adv_preds3 = sess.run(adv_preds_op3, feed_dict={x_op: xs})\n",
        "      preds = [np.argmax(adv_preds0, axis=1), np.argmax(adv_preds1, axis=1), np.argmax(adv_preds2, axis=1), np.argmax(adv_preds3, axis=1)]\n",
        "      if not singleModel:\n",
        "        finalPred = Ensembler(preds)\n",
        "      else:\n",
        "        finalPred = np.argmax(adv_preds1, axis=1)\n",
        "      correct += (finalPred == ys.cpu().detach().numpy()).sum()\n",
        "      total += test_loader.batch_size\n",
        "\n",
        "    acc = float(correct) / total\n",
        "    print('Adv accuracy: {:.3f}％'.format(acc * 100))\n",
        "    report.clean_train_adv_eval = acc\n",
        "    return report\n",
        "\n",
        "def mnist_test(nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE,\n",
        "               train_end=-1, test_end=-1, learning_rate=LEARNING_RATE, \n",
        "               torch_model=None, test_model1=None, test_model2=None, test_model3=None, \n",
        "               training=1, testEval=0, testAttack=0):\n",
        "  \"\"\"\n",
        "  MNIST cleverhans tutorial\n",
        "  :param nb_epochs: number of epochs to train model\n",
        "  :param batch_size: size of training batches\n",
        "  :param learning_rate: learning rate for training\n",
        "  :return: an AccuracyReport object\n",
        "  \"\"\"\n",
        "      # Train a pytorch MNIST model\n",
        "  if torch.cuda.is_available():\n",
        "    torch_model = torch_model.cuda()\n",
        "\n",
        "  report = AccuracyReport()\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      datasets.MNIST('data', train=True, download=True,\n",
        "                     transform=transforms.ToTensor()),\n",
        "      batch_size=batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "      batch_size=batch_size)\n",
        "  if training:\n",
        "    train(torch_model, train_loader, test_loader, nb_epochs, batch_size, train_end, test_end, learning_rate)\n",
        "  \n",
        "  if testEval:\n",
        "    print(\"Test Clean on same model\")\n",
        "    eval(torch_model, torch_model, torch_model, torch_model, test_loader, report, 1)\n",
        "    print(\"Test Clean on ensembled model\")\n",
        "    eval(torch_model, test_model1, test_model2, test_model3, test_loader, report, 0)\n",
        "  \n",
        "  # Evaluate on clean data\n",
        "  \n",
        "  if testAttack:\n",
        "    print(\"Test FGSM on the same model\")\n",
        "    report = AttackOnModel(torch_model, torch_model, torch_model, torch_model, test_loader, report, 1)\n",
        "    print(\"Test FGSM on ensembled model\")\n",
        "    report = AttackOnModel(torch_model, test_model1, test_model2, test_model3, test_loader, report, 0)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y03cDKDQuIsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Experiment with 3 models\n",
        "# model1 = LeNet5()\n",
        "# model2 = ResNet1()\n",
        "# model3 = VGGNet()\n",
        "\n",
        "# # Experiment 1\n",
        "# model1 = PytorchMnistModel()\n",
        "# model2 = LeNet5()\n",
        "# model3 = ResNet1()\n",
        "# model4 = VGGNet()\n",
        "\n",
        "# #Experiment 2\n",
        "# model1 = ResNet1()\n",
        "# model2 = ResNet1()\n",
        "# model3 = ResNet1()\n",
        "# model4 = ResNet1()\n",
        "\n",
        "# #Experiment 3\n",
        "# model1 = ResNet1()\n",
        "# model2 = ResNet2()\n",
        "# model3 = ResNet3()\n",
        "# model4 = ResNet4()\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVBzDfM6uOGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "ebaec243-cdc7-481d-8b06-6fccbfa1d27f"
      },
      "source": [
        "#train the two models\n",
        "print('Running Model 1')\n",
        "mnist_test(nb_epochs=4, batch_size=128, learning_rate=0.001, torch_model=model1, training=1, testAttack=0)\n",
        "print('Running Model 2')\n",
        "mnist_test(nb_epochs=4, batch_size=128, learning_rate=0.001, torch_model=model2, training=1, testAttack=0)\n",
        "print('Running Model 3')\n",
        "mnist_test(nb_epochs=4, batch_size=128, learning_rate=0.001, torch_model=model3, training=1, testAttack=0)\n",
        "# print('Running Model 4')\n",
        "# mnist_test(nb_epochs=4, batch_size=128, learning_rate=0.001, torch_model=model4, training=1, testAttack=0)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Model 1\n",
            "[125] Training accuracy: 73.71%\n",
            "[250] Training accuracy: 91.95%\n",
            "[375] Training accuracy: 95.47%\n",
            "[500] Training accuracy: 95.86%\n",
            "[625] Training accuracy: 96.90%\n",
            "[750] Training accuracy: 97.08%\n",
            "[875] Training accuracy: 97.42%\n",
            "[1000] Training accuracy: 97.59%\n",
            "[1125] Training accuracy: 97.78%\n",
            "[1250] Training accuracy: 98.07%\n",
            "[1375] Training accuracy: 97.87%\n",
            "[1500] Training accuracy: 98.11%\n",
            "[1625] Training accuracy: 98.64%\n",
            "[1750] Training accuracy: 98.28%\n",
            "[1875] Training accuracy: 98.26%\n",
            "Running Model 2\n",
            "[125] Training accuracy: 85.26%\n",
            "[250] Training accuracy: 96.23%\n",
            "[375] Training accuracy: 97.63%\n",
            "[500] Training accuracy: 97.65%\n",
            "[625] Training accuracy: 98.24%\n",
            "[750] Training accuracy: 98.51%\n",
            "[875] Training accuracy: 98.39%\n",
            "[1000] Training accuracy: 98.41%\n",
            "[1125] Training accuracy: 98.71%\n",
            "[1250] Training accuracy: 98.96%\n",
            "[1375] Training accuracy: 98.89%\n",
            "[1500] Training accuracy: 98.80%\n",
            "[1625] Training accuracy: 99.05%\n",
            "[1750] Training accuracy: 98.92%\n",
            "[1875] Training accuracy: 99.01%\n",
            "Running Model 3\n",
            "[125] Training accuracy: 86.32%\n",
            "[250] Training accuracy: 97.01%\n",
            "[375] Training accuracy: 97.99%\n",
            "[500] Training accuracy: 97.96%\n",
            "[625] Training accuracy: 98.58%\n",
            "[750] Training accuracy: 98.75%\n",
            "[875] Training accuracy: 98.66%\n",
            "[1000] Training accuracy: 98.67%\n",
            "[1125] Training accuracy: 99.08%\n",
            "[1250] Training accuracy: 99.08%\n",
            "[1375] Training accuracy: 99.06%\n",
            "[1500] Training accuracy: 98.98%\n",
            "[1625] Training accuracy: 99.32%\n",
            "[1750] Training accuracy: 99.21%\n",
            "[1875] Training accuracy: 99.35%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6vouuhSaEuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f77f2d25-b9b5-4b12-d716-8778731da96f"
      },
      "source": [
        "#test clean\n",
        "print('Testing model 1 clean')\n",
        "mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model1, \n",
        "           test_model1=model2, test_model2=model3, test_model3=model4, training=0, testEval=1, testAttack=0)\n",
        "print('Testing model 2 clean')\n",
        "mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model2, \n",
        "           test_model1=model1, test_model2=model3, test_model3=model4, training=0, testEval=1, testAttack=0)\n",
        "print('Testing model 3 clean')\n",
        "mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model3, \n",
        "           test_model1=model1, test_model2=model2, test_model3=model4, training=0, testEval=1, testAttack=0)\n",
        "# print('Testing model 4 clean')\n",
        "# mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model4, \n",
        "#            test_model1=model1, test_model2=model2, test_model3=model3, training=0, testEval=1, testAttack=0)\n",
        "\n",
        "# #test fgsm\n",
        "# print('Testing model 1 FGSM')\n",
        "# mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model1, \n",
        "#            test_model1=model2, test_model2=model3, test_model3=model4, training=0, testEval=1, testAttack=0)\n",
        "# print('Testing model 2 FGSM')\n",
        "# mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model2, \n",
        "#            test_model1=model1, test_model2=model3, test_model3=model4, training=0, testEval=1, testAttack=0)\n",
        "# print('Testing model 3 FGSM')\n",
        "# mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model3, \n",
        "#            test_model1=model1, test_model2=model2, test_model3=model4, training=0, testEval=1, testAttack=0)\n",
        "# print('Testing model 4 FGSM')\n",
        "# mnist_test(nb_epochs=2, batch_size=128, learning_rate=0.001, torch_model=model4, \n",
        "#            test_model1=model1, test_model2=model2, test_model3=model3, training=0, testEval=1, testAttack=0)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing model 1 clean\n",
            "Test Clean on same model\n",
            "Clean accuracy: 98.60%\n",
            "Test Clean on ensembled model\n",
            "Clean accuracy: 99.40%\n",
            "Testing model 2 clean\n",
            "Test Clean on same model\n",
            "Clean accuracy: 98.76%\n",
            "Test Clean on ensembled model\n",
            "Clean accuracy: 99.40%\n",
            "Testing model 3 clean\n",
            "Test Clean on same model\n",
            "Clean accuracy: 99.24%\n",
            "Test Clean on ensembled model\n",
            "Clean accuracy: 99.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta3SEwzFBKKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}