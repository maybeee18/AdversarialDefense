{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CleverhansTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPW0ntoF8OFb4ujI0k/Ejq3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttchengab/AdversarialDefense/blob/master/CleverhansTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT4dPiETNE7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d49ab1e-88c6-4928-e190-47237213ab65"
      },
      "source": [
        "!pip uninstall -y tensorflow-addons\n",
        "!pip uninstall -y tensorflow-datasets\n",
        "!pip uninstall -y tensorflow-estimator\n",
        "!pip uninstall -y tensorflow-gcs-config\n",
        "!pip uninstall -y tensorflow-hub\n",
        "!pip uninstall -y tensorflow-privacy\n",
        "!pip uninstall -y tensorflow-metadata\n",
        "!pip uninstall -y tensorflow-probability\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.14\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-addons-0.8.3:\n",
            "  Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Uninstalling tensorflow-datasets-2.1.0:\n",
            "  Successfully uninstalled tensorflow-datasets-2.1.0\n",
            "Uninstalling tensorflow-estimator-2.3.0:\n",
            "  Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Uninstalling tensorflow-gcs-config-2.3.0:\n",
            "  Successfully uninstalled tensorflow-gcs-config-2.3.0\n",
            "Uninstalling tensorflow-hub-0.8.0:\n",
            "  Successfully uninstalled tensorflow-hub-0.8.0\n",
            "Uninstalling tensorflow-privacy-0.2.2:\n",
            "  Successfully uninstalled tensorflow-privacy-0.2.2\n",
            "Uninstalling tensorflow-metadata-0.22.2:\n",
            "  Successfully uninstalled tensorflow-metadata-0.22.2\n",
            "Uninstalling tensorflow-probability-0.11.0:\n",
            "  Successfully uninstalled tensorflow-probability-0.11.0\n",
            "Uninstalling tensorflow-2.3.0:\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 44kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.30.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.9.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 45.5MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (49.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting cleverhans\n",
            "  Cloning https://github.com/tensorflow/cleverhans.git to /tmp/pip-install-im0nxicm/cleverhans\n",
            "  Running command git clone -q https://github.com/tensorflow/cleverhans.git /tmp/pip-install-im0nxicm/cleverhans\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 4.6MB/s \n",
            "\u001b[?25hCollecting pycodestyle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.2.2)\n",
            "Collecting mnist~=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.18.5)\n",
            "Collecting tensorflow-probability\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/b6/82ebba9b9ef3de851ea2c801a1542aa47821c62851ebabab565defa453b1/tensorflow_probability-0.11.0-py2.py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 92kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.1.5)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.15.0)\n",
            "Building wheels for collected packages: cleverhans\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cleverhans: filename=cleverhans-3.0.1-cp36-none-any.whl size=262573 sha256=7f9b7b676aee49d08483efdfe361048495e109341d5af47b653619a1842e5f69\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dtlhhcu4/wheels/6e/59/ec/723a6f654aaf62c8c40f0f0850fdf71a4948598697f56c3bfa\n",
            "Successfully built cleverhans\n",
            "Installing collected packages: nose, pycodestyle, mnist, tensorflow-probability, cleverhans\n",
            "Successfully installed cleverhans-3.0.1 mnist-0.2.2 nose-1.3.7 pycodestyle-2.6.0 tensorflow-probability-0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbZPnX53NMSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.compat import flags\n",
        "from cleverhans.train import train\n",
        "from cleverhans.dataset import MNIST\n",
        "from cleverhans.model import CallableModelWrapper\n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.utils import AccuracyReport\n",
        "from cleverhans.utils_tf import model_eval\n",
        "from cleverhans.utils_pytorch import convert_pytorch_model_to_tf\n",
        "\n",
        "#change to completely pytorch\n",
        "from cleverhans.future.torch.attacks.fast_gradient_method import fast_gradient_method\n",
        "\n",
        "#keras libraries, remove later\n",
        "# from cleverhans.utils_keras import cnn_model\n",
        "# from cleverhans.utils_keras import KerasModelWrapper"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RoumovAampC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TFG7ecwZvUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_size:int, hidden_size:int, out_size:int):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_size, hidden_size, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(hidden_size, out_size, 3, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(out_size)\n",
        "    \n",
        "    def convblock(self, x):\n",
        "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
        "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x): return x + self.convblock(x) # skip connection\n",
        "\n",
        "class ResNet1(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 16)\n",
        "        self.res2 = ResBlock(16, 32, 16)\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #16x28x28\n",
        "        x = self.res2(x) \n",
        "        #16x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #16x14x14\n",
        "        x = x.view(-1, 16*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class ResNet2(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 12)\n",
        "        self.res2 = ResBlock(12, 16, 12)\n",
        "        self.fc1 = nn.Linear(12 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #32x28x28\n",
        "        x = self.res2(x) \n",
        "        #16x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #16x14x14\n",
        "        x = x.view(-1, 12*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class ResNet3(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 32)\n",
        "        self.fc1 = nn.Linear(32 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #32x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #32x14x14\n",
        "        x = x.view(-1, 32*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class ResNet4(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(1, 8, 16)\n",
        "        self.res2 = ResBlock(16, 32, 16)\n",
        "        self.res3 = ResBlock(16, 8, 16)\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #1x28x28\n",
        "        x = self.res1(x)\n",
        "        #16x28x28\n",
        "        x = self.res2(x) \n",
        "        #32x28x28\n",
        "        x = self.res3(x)\n",
        "        #16x28x28\n",
        "        x = F.max_pool2d(F.relu(x), 2)\n",
        "        #16x14x14\n",
        "        x = x.view(-1, 16*14*14)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "\n",
        "class LeNet5(torch.nn.Module):          \n",
        "     \n",
        "    def __init__(self):     \n",
        "        super(LeNet5, self).__init__()\n",
        "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5, padding=2)\n",
        "        # Max-pooling\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "        # Convolution\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)   \n",
        "        self.fc2 = nn.Linear(120, 84)       \n",
        "        self.fc3 = nn.Linear(84, 10)    \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = F.relu(self.conv1(x))  \n",
        "        # max-pooling with 2x2 grid \n",
        "        x = F.max_pool2d(x, 2) \n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # max-pooling with 2x2 grid\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
        "        # read through https://stackoverflow.com/a/42482819/7551231\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        # FC-1, then perform ReLU non-linearity\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # FC-2, then perform ReLU non-linearity\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # FC-3\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return F.log_softmax(x,dim=-1)\n",
        "\n",
        "class PytorchMnistModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PytorchMnistModel, self).__init__()\n",
        "    # input is 28x28\n",
        "    # padding=2 for same padding\n",
        "    self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
        "    # feature map size is 14*14 by pooling\n",
        "    # padding=2 for same padding\n",
        "    self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "    # feature map size is 7*7 by pooling\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = x.view(-1, 64 * 7 * 7)  # reshape Variable\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=-1)\n",
        "  \n",
        "class FCNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FCNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 28, 5)\n",
        "    self.conv2 = nn.Conv2d(28, 42, 3, padding = 1)\n",
        "    self.fc1 = nn.Linear(42*6*6, 512)\n",
        "    self.fc2 = nn.Linear(512, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #1, 28, 28\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "    #28, 12, 12\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    #42, 6, 6\n",
        "    x = x.view(-1, 42*6*6)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "   def __init__(self):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.conv11 = nn.Conv2d(1, 64, 3)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
        "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
        "        self.fc1 = nn.Linear(128 * 2 * 2, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "   def forward(self, x):\n",
        "       #1, 28, 28\n",
        "       x = F.relu(self.conv11(x))\n",
        "       #64, 26, 26\n",
        "       x = F.relu(self.conv12(x))\n",
        "       #64, 24, 24\n",
        "       x = F.max_pool2d(x, (2,2))\n",
        "       #64, 12, 12\n",
        "       x = F.relu(self.conv21(x))\n",
        "       #128, 10, 10\n",
        "       x = F.relu(self.conv22(x))\n",
        "       #128, 8, 8\n",
        "       x = F.max_pool2d(x, (2,2))\n",
        "       #128, 4, 4\n",
        "       x = F.max_pool2d(x, (2,2))\n",
        "       #128, 2, 2\n",
        "       x = x.view(-1, 128 * 2 * 2)\n",
        "       x = F.relu(self.fc1(x))\n",
        "       x = self.fc2(x)\n",
        "       return F.log_softmax(x, dim=-1)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLpswmDdb--O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1e63cc1e-a8c7-4c5e-fee9-569b304b12a2"
      },
      "source": [
        "# Summary to check torch model structure\n",
        "from torchsummary import summary\n",
        "resnet = FCNet()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "resnet = resnet.to(device)\n",
        "summary(resnet, input_size=(1, 28, 28))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 28, 24, 24]             728\n",
            "            Conv2d-2           [-1, 42, 12, 12]          10,626\n",
            "            Linear-3                  [-1, 512]         774,656\n",
            "            Linear-4                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 791,140\n",
            "Trainable params: 791,140\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.17\n",
            "Params size (MB): 3.02\n",
            "Estimated Total Size (MB): 3.19\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSdErV8LNT8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = flags.FLAGS\n",
        "NB_EPOCHS = 2\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = .001\n",
        "\n",
        "def trainTorch(torch_model, train_loader, test_loader,\n",
        "        nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE, train_end=-1, test_end=-1, learning_rate=LEARNING_RATE):\n",
        "\n",
        "\n",
        "    # Truncate the datasets so that our test run more quickly\n",
        "  #   train_loader.dataset.train_data = train_loader.dataset.train_data[:train_end]\n",
        "  #   test_loader.dataset.test_data = test_loader.dataset.test_data[:test_end]\n",
        "\n",
        "    # Train our model\n",
        "    optimizer = optim.Adam(torch_model.parameters(), lr=learning_rate)\n",
        "    train_loss = []\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    step = 0\n",
        "    # breakstep = 0\n",
        "    for _epoch in range(nb_epochs):\n",
        "      # if breakstep == 2:\n",
        "      #     # print(\"break all!\")\n",
        "      #     break\n",
        "      for xs, ys in train_loader:\n",
        "        xs, ys = Variable(xs), Variable(ys)\n",
        "        if torch.cuda.is_available():\n",
        "          xs, ys = xs.cuda(), ys.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        preds = torch_model(xs)\n",
        "        loss = F.nll_loss(preds, ys)\n",
        "        loss.backward()  # calc gradients\n",
        "        train_loss.append(loss.data.item())\n",
        "        optimizer.step()  # update gradients\n",
        "\n",
        "        preds_np = preds.cpu().detach().numpy()\n",
        "        correct += (np.argmax(preds_np, axis=1) == ys.cpu().detach().numpy()).sum()\n",
        "        total += train_loader.batch_size\n",
        "        step += 1\n",
        "        if total % 1000 == 0:\n",
        "          acc = float(correct) / total\n",
        "          print('[%s] Training accuracy: %.2f%%' % (step, acc * 100))\n",
        "          total = 0\n",
        "          correct = 0\n",
        "          # breakstep += 1\n",
        "          # if breakstep == 2:\n",
        "          #     # print(\"break!\")\n",
        "          #     break\n",
        "    \n",
        "def Ensembler(preds):\n",
        "    finalPred = np.zeros(len(preds[0]))\n",
        "    for i in range(len(preds[0])):\n",
        "      scoreList = np.zeros(10)\n",
        "      for pred in preds:\n",
        "        scoreList[pred[i]] += 1\n",
        "      finalPred[i] = np.argmax(scoreList)\n",
        "    return finalPred\n",
        "\n",
        "def evalClean(model1=None, model2=None, model3=None, model4=None, test_loader=None, report=None, singleModel=0):\n",
        "    if singleModel:\n",
        "      print(\"Evaluating single model results on clean data\")\n",
        "    else:\n",
        "      print(\"Evaluating the ensembled method on clean data\")\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      model1.eval()\n",
        "      if not singleModel:\n",
        "        model2.eval()\n",
        "        model3.eval()\n",
        "        # only when 4 models\n",
        "        # model4.eval()\n",
        "      for xs, ys in test_loader:\n",
        "        xs, ys = Variable(xs), Variable(ys)\n",
        "        if torch.cuda.is_available():\n",
        "          xs, ys = xs.cuda(), ys.cuda()\n",
        "        preds1 = model1(xs)\n",
        "        preds_np1 = preds1.cpu().detach().numpy()\n",
        "        if not singleModel:\n",
        "          preds2 = model2(xs)\n",
        "          preds_np2 = preds2.cpu().detach().numpy()\n",
        "          preds3 = model3(xs)\n",
        "          preds_np3 = preds3.cpu().detach().numpy()\n",
        "          # only when 4 models\n",
        "          # preds4 = model4(xs)\n",
        "          # preds_np4 = preds4.cpu().detach().numpy()    \n",
        "          #preds for 3 and 4\n",
        "          preds = [np.argmax(preds_np1, axis=1), np.argmax(preds_np2, axis=1), np.argmax(preds_np3, axis=1)]\n",
        "          # preds = [np.argmax(preds_np1, axis=1), np.argmax(preds_np2, axis=1), np.argmax(preds_np3, axis=1), np.argmax(preds_np4, axis=1)]\n",
        "          finalPred = Ensembler(preds)\n",
        "        else:\n",
        "          finalPred = np.argmax(preds_np1, axis=1)\n",
        "        correct += (finalPred == ys.cpu().detach().numpy()).sum()\n",
        "        total += len(xs)\n",
        "    acc = float(correct) / total\n",
        "    report.clean_train_clean_eval = acc\n",
        "    print('Clean accuracy: %.2f%%' % (acc * 100))\n",
        "\n",
        "    return report\n",
        "\n",
        "def evalAdvAttack(fgsm_model=None, model2=None, model3=None, model4=None, test_loader=None, singleModel=0):\n",
        "    # fgsm_model\n",
        "    # fast_gradient_method(model_fn, x, eps, norm, clip_min=None, clip_max=None, y=None, targeted=False, sanity_checks=False):\n",
        "    if singleModel:\n",
        "      print(\"Evaluating single model results on adv data\")\n",
        "    else:\n",
        "      print(\"Evaluating the ensembled method on adv data\")\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    fgsm_model.eval()\n",
        "    if not singleModel:\n",
        "      model2.eval()\n",
        "      model3.eval()\n",
        "      # only when 4 models\n",
        "      # model4.eval()\n",
        "    \n",
        "    for xs, ys in test_loader:\n",
        "      if torch.cuda.is_available():\n",
        "        xs, ys = xs.cuda(), ys.cuda()\n",
        "      #pytorch fast gradient method\n",
        "      xs = fast_gradient_method(fgsm_model, xs, eps=0.3, norm=np.inf, clip_min=0., clip_max=1.)\n",
        "      # xs = fast_gradient_method(fgsm_model, xs, eps=0.3, norm=np.inf)\n",
        "      xs, ys = Variable(xs), Variable(ys)\n",
        "      preds1 = fgsm_model(xs)\n",
        "      # loss = F.nll_loss(preds1, ys)\n",
        "      # loss.backward()  # calc gradients\n",
        "      # train_loss.append(loss.data.item())\n",
        "      # optimizer.step()  # update gradients\n",
        "      preds_np1 = preds1.cpu().detach().numpy()\n",
        "      if not singleModel:\n",
        "          preds2 = model2(xs)\n",
        "          preds_np2 = preds2.cpu().detach().numpy()\n",
        "          preds3 = model3(xs)\n",
        "          preds_np3 = preds3.cpu().detach().numpy()\n",
        "          # only when 4 models\n",
        "          # preds4 = model4(xs)\n",
        "          # preds_np4 = preds4.cpu().detach().numpy()  \n",
        "          #preds for 3 and 4\n",
        "          preds = [np.argmax(preds_np1, axis=1), np.argmax(preds_np2, axis=1), np.argmax(preds_np3, axis=1)]\n",
        "          # preds = [np.argmax(preds_np1, axis=1), np.argmax(preds_np2, axis=1), np.argmax(preds_np3, axis=1), np.argmax(preds_np4, axis=1)]\n",
        "          finalPred = Ensembler(preds)\n",
        "      else:\n",
        "          finalPred = np.argmax(preds_np1, axis=1)\n",
        "      correct += (finalPred == ys.cpu().detach().numpy()).sum()\n",
        "      total += test_loader.batch_size\n",
        "    acc = float(correct) / total\n",
        "    print('Adv accuracy: {:.3f}％'.format(acc * 100))\n",
        "      # break\n",
        "\n",
        "def evalCombined(model1, model2, model3, test_loader, report):\n",
        "    # This function evaluates the main model (model 1) and the ensembled data (model1/2/3) on clean and adv data\n",
        "\n",
        "    # Evaluate single model on clean data\n",
        "    evalClean(model1=model1, test_loader=test_loader, report=report, singleModel=1)\n",
        "    # Evaluate ensembled model on clean data\n",
        "    evalClean(model1=model1, model2=model2, model3=model3, test_loader=test_loader, report=report, singleModel=0)\n",
        "    # Evaluate single model on adversarial attack\n",
        "    evalAdvAttack(fgsm_model=model1, test_loader=test_loader, singleModel=1)\n",
        "    # Evaluate ensembled model on adversarial attack\n",
        "    evalAdvAttack(fgsm_model=model1, model2=model2, model3=model3, test_loader=test_loader, singleModel=0)\n",
        "\n",
        "def advTrain(torch_model, train_loader, test_loader,\n",
        "        nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE, train_end=-1, test_end=-1, learning_rate=LEARNING_RATE):\n",
        "    optimizer = optim.Adam(torch_model.parameters(), lr=learning_rate)\n",
        "    train_loss = []\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    totalAdv = 0\n",
        "    correctAdv = 0\n",
        "    step = 0\n",
        "    # breakstep = 0\n",
        "    for _epoch in range(nb_epochs):\n",
        "      for xs, ys in train_loader:\n",
        "        #Normal Training\n",
        "        xs, ys = Variable(xs), Variable(ys)\n",
        "        if torch.cuda.is_available():\n",
        "          xs, ys = xs.cuda(), ys.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        preds = torch_model(xs)\n",
        "        loss = F.nll_loss(preds, ys)\n",
        "        loss.backward()  # calc gradients\n",
        "        train_loss.append(loss.data.item())\n",
        "        optimizer.step()  # update gradients\n",
        "        preds_np = preds.cpu().detach().numpy()\n",
        "        correct += (np.argmax(preds_np, axis=1) == ys.cpu().detach().numpy()).sum()\n",
        "        total += train_loader.batch_size\n",
        "\n",
        "        #Adversarial Training\n",
        "        xs = fast_gradient_method(torch_model, xs, eps=0.3, norm=np.inf, clip_min=0., clip_max=1.)\n",
        "        xs, ys = Variable(xs), Variable(ys)\n",
        "        if torch.cuda.is_available():\n",
        "          xs, ys = xs.cuda(), ys.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        preds = torch_model(xs)\n",
        "        loss = F.nll_loss(preds, ys)\n",
        "        loss.backward()  # calc gradients\n",
        "        train_loss.append(loss.data.item())\n",
        "        optimizer.step()  # update gradients\n",
        "        preds_np = preds.cpu().detach().numpy()\n",
        "        correctAdv += (np.argmax(preds_np, axis=1) == ys.cpu().detach().numpy()).sum()\n",
        "        totalAdv += train_loader.batch_size\n",
        "        \n",
        "        step += 1\n",
        "        if total % 1000 == 0:\n",
        "          acc = float(correct) / total\n",
        "          print('[%s] Clean Training accuracy: %.2f%%' % (step, acc * 100))\n",
        "          total = 0\n",
        "          correct = 0\n",
        "          accAdv = float(correctAdv) / totalAdv\n",
        "          print('[%s] Adv Training accuracy: %.2f%%' % (step, accAdv * 100))\n",
        "          totalAdv = 0\n",
        "          correctAdv = 0\n",
        "\n",
        "\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y03cDKDQuIsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment with 3 models\n",
        "# model1 = LeNet5()\n",
        "# model2 = FCNet()\n",
        "# model3 = PytorchMnistModel()\n",
        "\n",
        "# # Experiment 1\n",
        "# model1 = PytorchMnistModel()\n",
        "# model2 = LeNet5()\n",
        "# model3 = ResNet1()\n",
        "# model4 = VGGNet()\n",
        "\n",
        "#Experiment 2\n",
        "model1 = ResNet1()\n",
        "model2 = ResNet1()\n",
        "model3 = ResNet1()\n",
        "# model4 = ResNet1()\n",
        "\n",
        "# #Experiment 3\n",
        "# model1 = ResNet1()\n",
        "# model2 = ResNet2()\n",
        "# model3 = ResNet3()\n",
        "# model4 = ResNet4()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model1 = model1.cuda()\n",
        "  model2 = model2.cuda()\n",
        "  model3 = model3.cuda()\n",
        "  #model4 = model4.cuda()\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0EWyi5_VW0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize train loader and test loader and hyperparameters\n",
        "nb_epochs = 4\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "train_end = -1\n",
        "test_end = -1\n",
        "report = AccuracyReport()\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP_c73g3XFws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "0e970bd4-8a5d-4016-ba54-99a6400301b5"
      },
      "source": [
        "#Training\n",
        "print(\"Training Model 1\")\n",
        "trainTorch(model1, train_loader, test_loader, nb_epochs, batch_size, train_end, test_end, learning_rate)\n",
        "print(\"Training Model 2\")\n",
        "trainTorch(model2, train_loader, test_loader, nb_epochs, batch_size, train_end, test_end, learning_rate)\n",
        "print(\"Training Model 3\")\n",
        "trainTorch(model3, train_loader, test_loader, nb_epochs, batch_size, train_end, test_end, learning_rate)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model 1\n",
            "[125] Training accuracy: 85.12%\n",
            "[250] Training accuracy: 96.45%\n",
            "[375] Training accuracy: 97.59%\n",
            "[500] Training accuracy: 97.74%\n",
            "[625] Training accuracy: 98.45%\n",
            "[750] Training accuracy: 98.24%\n",
            "[875] Training accuracy: 98.37%\n",
            "[1000] Training accuracy: 98.38%\n",
            "[1125] Training accuracy: 99.05%\n",
            "[1250] Training accuracy: 98.91%\n",
            "[1375] Training accuracy: 98.78%\n",
            "[1500] Training accuracy: 98.88%\n",
            "[1625] Training accuracy: 99.23%\n",
            "[1750] Training accuracy: 98.94%\n",
            "[1875] Training accuracy: 98.90%\n",
            "Training Model 2\n",
            "[125] Training accuracy: 85.81%\n",
            "[250] Training accuracy: 96.55%\n",
            "[375] Training accuracy: 97.34%\n",
            "[500] Training accuracy: 97.67%\n",
            "[625] Training accuracy: 98.38%\n",
            "[750] Training accuracy: 98.28%\n",
            "[875] Training accuracy: 98.29%\n",
            "[1000] Training accuracy: 98.64%\n",
            "[1125] Training accuracy: 98.72%\n",
            "[1250] Training accuracy: 98.72%\n",
            "[1375] Training accuracy: 98.78%\n",
            "[1500] Training accuracy: 98.74%\n",
            "[1625] Training accuracy: 99.08%\n",
            "[1750] Training accuracy: 99.01%\n",
            "[1875] Training accuracy: 99.04%\n",
            "Training Model 3\n",
            "[125] Training accuracy: 86.62%\n",
            "[250] Training accuracy: 96.56%\n",
            "[375] Training accuracy: 97.31%\n",
            "[500] Training accuracy: 97.78%\n",
            "[625] Training accuracy: 98.29%\n",
            "[750] Training accuracy: 98.12%\n",
            "[875] Training accuracy: 98.37%\n",
            "[1000] Training accuracy: 98.36%\n",
            "[1125] Training accuracy: 98.74%\n",
            "[1250] Training accuracy: 98.85%\n",
            "[1375] Training accuracy: 98.61%\n",
            "[1500] Training accuracy: 98.95%\n",
            "[1625] Training accuracy: 99.20%\n",
            "[1750] Training accuracy: 99.08%\n",
            "[1875] Training accuracy: 98.89%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHqh3xsZXeqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "ec815c3d-e8bb-40d1-e00a-9ed630758309"
      },
      "source": [
        "print(\"Evaluation before training\")\n",
        "print(\"Evaluating model 1\")\n",
        "evalCombined(model1=model1, model2=model2, model3=model3, test_loader=test_loader,report=report)\n",
        "print(\"Evaluating model 2\")\n",
        "evalCombined(model1=model2, model2=model1, model3=model3, test_loader=test_loader,report=report)\n",
        "print(\"Evaluating model 3\")\n",
        "evalCombined(model1=model3, model2=model1, model3=model2, test_loader=test_loader,report=report)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation before training\n",
            "Evaluating model 1\n",
            "Evaluating single model results on clean data\n",
            "Clean accuracy: 98.80%\n",
            "Evaluating the ensembled method on clean data\n",
            "Clean accuracy: 99.27%\n",
            "Evaluating single model results on adv data\n",
            "Adv accuracy: 1.483％\n",
            "Evaluating the ensembled method on adv data\n",
            "Adv accuracy: 13.360％\n",
            "Evaluating model 2\n",
            "Evaluating single model results on clean data\n",
            "Clean accuracy: 99.01%\n",
            "Evaluating the ensembled method on clean data\n",
            "Clean accuracy: 99.27%\n",
            "Evaluating single model results on adv data\n",
            "Adv accuracy: 3.016％\n",
            "Evaluating the ensembled method on adv data\n",
            "Adv accuracy: 7.941％\n",
            "Evaluating model 3\n",
            "Evaluating single model results on clean data\n",
            "Clean accuracy: 99.05%\n",
            "Evaluating the ensembled method on clean data\n",
            "Clean accuracy: 99.27%\n",
            "Evaluating single model results on adv data\n",
            "Adv accuracy: 2.373％\n",
            "Evaluating the ensembled method on adv data\n",
            "Adv accuracy: 11.116％\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Qt9V2IdZ9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87c0021b-dd1f-4123-b180-fb0f9dd3df72"
      },
      "source": [
        "print(\"Perform Adversarial Training\")\n",
        "advTrain(model1, train_loader, test_loader, nb_epochs, batch_size, train_end, test_end, learning_rate)\n",
        "advTrain(model2, train_loader, test_loader, nb_epochs, batch_size, train_end, test_end, learning_rate)\n",
        "advTrain(model3, train_loader, test_loader, nb_epochs, batch_size, train_end, test_end, learning_rate)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perform Adversarial Training\n",
            "[125] Clean Training accuracy: 97.07%\n",
            "[125] Adv Training accuracy: 35.04%\n",
            "[250] Clean Training accuracy: 97.56%\n",
            "[250] Adv Training accuracy: 61.61%\n",
            "[375] Clean Training accuracy: 97.98%\n",
            "[375] Adv Training accuracy: 72.96%\n",
            "[500] Clean Training accuracy: 97.83%\n",
            "[500] Adv Training accuracy: 80.51%\n",
            "[625] Clean Training accuracy: 98.26%\n",
            "[625] Adv Training accuracy: 84.45%\n",
            "[750] Clean Training accuracy: 98.20%\n",
            "[750] Adv Training accuracy: 86.99%\n",
            "[875] Clean Training accuracy: 98.11%\n",
            "[875] Adv Training accuracy: 87.79%\n",
            "[1000] Clean Training accuracy: 98.36%\n",
            "[1000] Adv Training accuracy: 89.32%\n",
            "[1125] Clean Training accuracy: 98.47%\n",
            "[1125] Adv Training accuracy: 89.33%\n",
            "[1250] Clean Training accuracy: 98.63%\n",
            "[1250] Adv Training accuracy: 91.11%\n",
            "[1375] Clean Training accuracy: 98.36%\n",
            "[1375] Adv Training accuracy: 91.41%\n",
            "[1500] Clean Training accuracy: 98.66%\n",
            "[1500] Adv Training accuracy: 92.33%\n",
            "[1625] Clean Training accuracy: 98.71%\n",
            "[1625] Adv Training accuracy: 92.57%\n",
            "[1750] Clean Training accuracy: 98.67%\n",
            "[1750] Adv Training accuracy: 92.07%\n",
            "[1875] Clean Training accuracy: 98.72%\n",
            "[1875] Adv Training accuracy: 92.59%\n",
            "[125] Clean Training accuracy: 98.41%\n",
            "[125] Adv Training accuracy: 53.59%\n",
            "[250] Clean Training accuracy: 98.66%\n",
            "[250] Adv Training accuracy: 78.84%\n",
            "[375] Clean Training accuracy: 98.88%\n",
            "[375] Adv Training accuracy: 85.03%\n",
            "[500] Clean Training accuracy: 98.79%\n",
            "[500] Adv Training accuracy: 88.21%\n",
            "[625] Clean Training accuracy: 99.03%\n",
            "[625] Adv Training accuracy: 90.22%\n",
            "[750] Clean Training accuracy: 99.01%\n",
            "[750] Adv Training accuracy: 92.15%\n",
            "[875] Clean Training accuracy: 99.08%\n",
            "[875] Adv Training accuracy: 92.63%\n",
            "[1000] Clean Training accuracy: 99.02%\n",
            "[1000] Adv Training accuracy: 93.41%\n",
            "[1125] Clean Training accuracy: 99.41%\n",
            "[1125] Adv Training accuracy: 94.77%\n",
            "[1250] Clean Training accuracy: 99.28%\n",
            "[1250] Adv Training accuracy: 95.69%\n",
            "[1375] Clean Training accuracy: 99.23%\n",
            "[1375] Adv Training accuracy: 95.28%\n",
            "[1500] Clean Training accuracy: 99.27%\n",
            "[1500] Adv Training accuracy: 95.26%\n",
            "[1625] Clean Training accuracy: 99.57%\n",
            "[1625] Adv Training accuracy: 96.21%\n",
            "[1750] Clean Training accuracy: 99.46%\n",
            "[1750] Adv Training accuracy: 95.86%\n",
            "[1875] Clean Training accuracy: 99.57%\n",
            "[1875] Adv Training accuracy: 96.10%\n",
            "[125] Clean Training accuracy: 98.78%\n",
            "[125] Adv Training accuracy: 60.71%\n",
            "[250] Clean Training accuracy: 99.13%\n",
            "[250] Adv Training accuracy: 84.87%\n",
            "[375] Clean Training accuracy: 99.09%\n",
            "[375] Adv Training accuracy: 91.32%\n",
            "[500] Clean Training accuracy: 98.94%\n",
            "[500] Adv Training accuracy: 92.44%\n",
            "[625] Clean Training accuracy: 99.42%\n",
            "[625] Adv Training accuracy: 93.81%\n",
            "[750] Clean Training accuracy: 99.40%\n",
            "[750] Adv Training accuracy: 94.74%\n",
            "[875] Clean Training accuracy: 99.33%\n",
            "[875] Adv Training accuracy: 95.28%\n",
            "[1000] Clean Training accuracy: 99.28%\n",
            "[1000] Adv Training accuracy: 96.34%\n",
            "[1125] Clean Training accuracy: 99.52%\n",
            "[1125] Adv Training accuracy: 96.53%\n",
            "[1250] Clean Training accuracy: 99.49%\n",
            "[1250] Adv Training accuracy: 97.11%\n",
            "[1375] Clean Training accuracy: 99.38%\n",
            "[1375] Adv Training accuracy: 96.70%\n",
            "[1500] Clean Training accuracy: 99.34%\n",
            "[1500] Adv Training accuracy: 97.54%\n",
            "[1625] Clean Training accuracy: 99.55%\n",
            "[1625] Adv Training accuracy: 97.92%\n",
            "[1750] Clean Training accuracy: 99.63%\n",
            "[1750] Adv Training accuracy: 97.76%\n",
            "[1875] Clean Training accuracy: 99.58%\n",
            "[1875] Adv Training accuracy: 97.56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irB0WieOepR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "28fc26e0-d6dc-4ba2-92e1-28c44ee2afb1"
      },
      "source": [
        "print(\"Evaluation before training\")\n",
        "print(\"Evaluating model 1\")\n",
        "evalCombined(model1=model1, model2=model2, model3=model3, test_loader=test_loader,report=report)\n",
        "print(\"Evaluating model 2\")\n",
        "evalCombined(model1=model2, model2=model1, model3=model3, test_loader=test_loader,report=report)\n",
        "print(\"Evaluating model 3\")\n",
        "evalCombined(model1=model3, model2=model1, model3=model2, test_loader=test_loader,report=report)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation before training\n",
            "Evaluating model 1\n",
            "Evaluating single model results on clean data\n",
            "Clean accuracy: 98.54%\n",
            "Evaluating the ensembled method on clean data\n",
            "Clean accuracy: 99.25%\n",
            "Evaluating single model results on adv data\n",
            "Adv accuracy: 90.180％\n",
            "Evaluating the ensembled method on adv data\n",
            "Adv accuracy: 87.955％\n",
            "Evaluating model 2\n",
            "Evaluating single model results on clean data\n",
            "Clean accuracy: 98.95%\n",
            "Evaluating the ensembled method on clean data\n",
            "Clean accuracy: 99.25%\n",
            "Evaluating single model results on adv data\n",
            "Adv accuracy: 94.531％\n",
            "Evaluating the ensembled method on adv data\n",
            "Adv accuracy: 88.430％\n",
            "Evaluating model 3\n",
            "Evaluating single model results on clean data\n",
            "Clean accuracy: 99.10%\n",
            "Evaluating the ensembled method on clean data\n",
            "Clean accuracy: 99.25%\n",
            "Evaluating single model results on adv data\n",
            "Adv accuracy: 96.994％\n",
            "Evaluating the ensembled method on adv data\n",
            "Adv accuracy: 87.876％\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta3SEwzFBKKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def AttackOnModel(fgsm_model=None, test_model1=None, test_model2=None, test_model3=None, test_loader=None, report=None, singleModel=0):\n",
        "#     # We use tf for evaluation on adversarial data\n",
        "#     sess = tf.Session()\n",
        "\n",
        "#     x_op = tf.placeholder(tf.float32, shape=(None, 1, 28, 28,))\n",
        "\n",
        "#     # Convert pytorch model to a tf_model and wrap it in cleverhans\n",
        "#     tf_model_fn = convert_pytorch_model_to_tf(fgsm_model)\n",
        "#     cleverhans_model = CallableModelWrapper(tf_model_fn, output_layer='logits')\n",
        "\n",
        "#     # Convert the testing models\n",
        "#     test_model_fn1 = convert_pytorch_model_to_tf(test_model1)\n",
        "#     test_model_fn2 = convert_pytorch_model_to_tf(test_model2)\n",
        "#     test_model_fn3 = convert_pytorch_model_to_tf(test_model3)\n",
        "\n",
        "#     # Create an FGSM attack\n",
        "#     fgsm_op = FastGradientMethod(cleverhans_model, sess=sess)\n",
        "#     fgsm_params = {'eps': 0.3,\n",
        "#                    'clip_min': 0.,\n",
        "#                    'clip_max': 1.}\n",
        "#     adv_x_op = fgsm_op.generate(x_op, **fgsm_params)\n",
        "\n",
        "#     adv_preds_op0 = tf_model_fn(adv_x_op)\n",
        "#     adv_preds_op1 = test_model_fn1(adv_x_op)\n",
        "#     adv_preds_op2 = test_model_fn2(adv_x_op)\n",
        "#     adv_preds_op3 = test_model_fn3(adv_x_op)\n",
        "#     # Run an evaluation of our model against fgsm\n",
        "#     total = 0\n",
        "#     correct = 0\n",
        "#     for xs, ys in test_loader:\n",
        "#       adv_preds0 = sess.run(adv_preds_op0, feed_dict={x_op: xs})\n",
        "#       adv_preds1 = sess.run(adv_preds_op1, feed_dict={x_op: xs})\n",
        "#       adv_preds2 = sess.run(adv_preds_op2, feed_dict={x_op: xs})\n",
        "#       adv_preds3 = sess.run(adv_preds_op3, feed_dict={x_op: xs})\n",
        "#       preds = [np.argmax(adv_preds0, axis=1), np.argmax(adv_preds1, axis=1), np.argmax(adv_preds2, axis=1), np.argmax(adv_preds3, axis=1)]\n",
        "#       if not singleModel:\n",
        "#         finalPred = Ensembler(preds)\n",
        "#       else:\n",
        "#         finalPred = np.argmax(adv_preds1, axis=1)\n",
        "#       correct += (finalPred == ys.cpu().detach().numpy()).sum()\n",
        "#       total += test_loader.batch_size\n",
        "\n",
        "#     acc = float(correct) / total\n",
        "#     print('Adv accuracy: {:.3f}％'.format(acc * 100))\n",
        "#     report.clean_train_adv_eval = acc\n",
        "#     # train(sess, loss, x_train, y_train, evaluate=evaluate, args=train_params, rng=rng, var_list=model.get_params())\n",
        "#     return report"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6-EYFdWbByE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}